{"version":3,"sources":["../../src/lib/langchain-adapter.ts","../../src/utils/openai.ts"],"sourcesContent":["import { ChatCompletionChunk } from \"@copilotkit/shared\";\nimport {\n  AIMessage,\n  BaseMessage,\n  BaseMessageChunk,\n  HumanMessage,\n  SystemMessage,\n} from \"@langchain/core/messages\";\nimport { IterableReadableStream } from \"@langchain/core/utils/stream\";\nimport { CopilotKitServiceAdapter } from \"../types\";\nimport { writeChatCompletionChunk, writeChatCompletionEnd } from \"../utils\";\n\nexport type LangChainMessageStream = IterableReadableStream<BaseMessageChunk>;\nexport type LangChainReturnType = LangChainMessageStream | BaseMessageChunk | string | AIMessage;\n\nexport class LangChainAdapter implements CopilotKitServiceAdapter {\n  constructor(private chainFn: (forwardedProps: any) => Promise<LangChainReturnType>) {}\n\n  async stream(forwardedProps: any): Promise<ReadableStream<any>> {\n    forwardedProps = this.transformProps(forwardedProps);\n\n    const result = await this.chainFn(forwardedProps);\n\n    // We support several types of return values from LangChain functions:\n\n    // 1. string\n    // Just send one chunk with the string as the content.\n    if (typeof result === \"string\") {\n      return new SingleChunkReadableStream(result);\n    }\n\n    // 2. AIMessage\n    // Send the content and function call of the AIMessage as the content of the chunk.\n    else if (\"content\" in result && typeof result.content === \"string\") {\n      return new SingleChunkReadableStream(result.content, result.additional_kwargs?.tool_calls);\n    }\n\n    // 3. BaseMessageChunk\n    // Send the content and function call of the AIMessage as the content of the chunk.\n    else if (\"lc_kwargs\" in result) {\n      return new SingleChunkReadableStream(result.lc_kwargs?.content, result.lc_kwargs?.tool_calls);\n    }\n\n    // 4. IterableReadableStream\n    // Stream the result of the LangChain function.\n    else if (\"getReader\" in result) {\n      return this.streamResult(result);\n    }\n\n    // TODO write function call result!\n\n    console.error(\"Invalid return type from LangChain function.\");\n    throw new Error(\"Invalid return type from LangChain function.\");\n  }\n\n  /**\n   * Transforms the props that are forwarded to the LangChain function.\n   * Currently this just transforms the messages to the format that LangChain expects.\n   *\n   * @param forwardedProps\n   * @returns {any}\n   */\n  private transformProps(forwardedProps: any) {\n    const forwardedPropsCopy = Object.assign({}, forwardedProps);\n\n    // map messages to langchain format\n    if (forwardedProps.messages && Array.isArray(forwardedProps.messages)) {\n      const newMessages: BaseMessage[] = [];\n      for (const message of forwardedProps.messages) {\n        if (message.role === \"user\") {\n          newMessages.push(new HumanMessage(message.content));\n        } else if (message.role === \"assistant\") {\n          newMessages.push(new AIMessage(message.content));\n        } else if (message.role === \"system\") {\n          newMessages.push(new SystemMessage(message.content));\n        }\n      }\n      forwardedPropsCopy.messages = newMessages;\n    }\n\n    return forwardedPropsCopy;\n  }\n\n  /**\n   * Reads from the LangChainMessageStream and converts the output to a ReadableStream.\n   *\n   * @param streamedChain\n   * @returns ReadableStream\n   */\n  streamResult(streamedChain: LangChainMessageStream): ReadableStream<any> {\n    let reader = streamedChain.getReader();\n\n    async function cleanup(controller?: ReadableStreamDefaultController<BaseMessageChunk>) {\n      if (controller) {\n        try {\n          controller.close();\n        } catch (_) {}\n      }\n      if (reader) {\n        try {\n          await reader.cancel();\n        } catch (_) {}\n      }\n    }\n\n    return new ReadableStream<any>({\n      async pull(controller) {\n        while (true) {\n          try {\n            const { done, value } = await reader.read();\n\n            if (done) {\n              writeChatCompletionEnd(controller);\n              await cleanup(controller);\n              return;\n            }\n\n            const toolCalls = value.lc_kwargs?.additional_kwargs?.tool_calls;\n            const content = value?.lc_kwargs?.content;\n            const chunk: ChatCompletionChunk = {\n              choices: [\n                {\n                  delta: {\n                    role: \"assistant\",\n                    content: content,\n                    ...(toolCalls ? { tool_calls: toolCalls } : {}),\n                  },\n                },\n              ],\n            };\n            writeChatCompletionChunk(controller, chunk);\n          } catch (error) {\n            controller.error(error);\n            await cleanup(controller);\n            return;\n          }\n        }\n      },\n      cancel() {\n        cleanup();\n      },\n    });\n  }\n}\n\n/**\n * A ReadableStream that only emits a single chunk.\n */\nclass SingleChunkReadableStream extends ReadableStream<any> {\n  constructor(content: string = \"\", toolCalls?: any) {\n    super({\n      start(controller) {\n        const chunk: ChatCompletionChunk = {\n          choices: [\n            {\n              delta: {\n                role: \"assistant\",\n                content,\n                ...(toolCalls ? { tool_calls: toolCalls } : {}),\n              },\n            },\n          ],\n        };\n        writeChatCompletionChunk(controller, chunk);\n        writeChatCompletionEnd(controller);\n\n        controller.close();\n      },\n      cancel() {},\n    });\n  }\n}\n","import { Message, ToolDefinition, ChatCompletionChunk, encodeResult } from \"@copilotkit/shared\";\n\nexport function writeChatCompletionChunk(\n  controller: ReadableStreamDefaultController<any>,\n  chunk: ChatCompletionChunk,\n) {\n  const payload = new TextEncoder().encode(\"data: \" + JSON.stringify(chunk) + \"\\n\\n\");\n  controller!.enqueue(payload);\n}\n\nexport function writeChatCompletionContent(\n  controller: ReadableStreamDefaultController<any>,\n  content: string = \"\",\n  toolCalls?: any,\n) {\n  const chunk: ChatCompletionChunk = {\n    choices: [\n      {\n        delta: {\n          role: \"assistant\",\n          content: content,\n          ...(toolCalls ? { tool_calls: toolCalls } : {}),\n        },\n      },\n    ],\n  };\n\n  writeChatCompletionChunk(controller, chunk);\n}\n\nexport function writeChatCompletionResult(\n  controller: ReadableStreamDefaultController<any>,\n  functionName: string,\n  result: any,\n) {\n  let resultString = encodeResult(result);\n\n  const chunk: ChatCompletionChunk = {\n    choices: [\n      {\n        delta: {\n          role: \"function\",\n          content: resultString,\n          name: functionName,\n        },\n      },\n    ],\n  };\n\n  writeChatCompletionChunk(controller, chunk);\n}\n\nexport function writeChatCompletionEnd(controller: ReadableStreamDefaultController<any>) {\n  const payload = new TextEncoder().encode(\"data: [DONE]\\n\\n\");\n  controller.enqueue(payload);\n}\n\nexport function limitOpenAIMessagesToTokenCount(\n  messages: Message[],\n  tools: ToolDefinition[],\n  maxTokens: number,\n): Message[] {\n  const result: Message[] = [];\n  const toolsNumTokens = countToolsTokens(tools);\n  if (toolsNumTokens > maxTokens) {\n    throw new Error(`Too many tokens in function definitions: ${toolsNumTokens} > ${maxTokens}`);\n  }\n  maxTokens -= toolsNumTokens;\n\n  for (const message of messages) {\n    if (message.role === \"system\") {\n      const numTokens = countMessageTokens(message);\n      maxTokens -= numTokens;\n\n      if (maxTokens < 0) {\n        throw new Error(\"Not enough tokens for system message.\");\n      }\n    }\n  }\n\n  let cutoff: boolean = false;\n\n  const reversedMessages = [...messages].reverse();\n  for (const message of reversedMessages) {\n    if (message.role === \"system\") {\n      result.unshift(message);\n      continue;\n    } else if (cutoff) {\n      continue;\n    }\n    let numTokens = countMessageTokens(message);\n    if (maxTokens < numTokens) {\n      cutoff = true;\n      continue;\n    }\n    result.unshift(message);\n    maxTokens -= numTokens;\n  }\n\n  return result;\n}\n\nexport function maxTokensForOpenAIModel(model: string): number {\n  return maxTokensByModel[model] || DEFAULT_MAX_TOKENS;\n}\n\nconst DEFAULT_MAX_TOKENS = 8192;\n\nconst maxTokensByModel: { [key: string]: number } = {\n  \"gpt-3.5-turbo\": 4097,\n  \"gpt-3.5-turbo-16k\": 16385,\n  \"gpt-4\": 8192,\n  \"gpt-4-1106-preview\": 8192,\n  \"gpt-4-32k\": 32768,\n  \"gpt-3.5-turbo-0301\": 4097,\n  \"gpt-4-0314\": 8192,\n  \"gpt-4-32k-0314\": 32768,\n  \"gpt-3.5-turbo-0613\": 4097,\n  \"gpt-4-0613\": 8192,\n  \"gpt-4-32k-0613\": 32768,\n  \"gpt-3.5-turbo-16k-0613\": 16385,\n};\n\nfunction countToolsTokens(functions: ToolDefinition[]): number {\n  if (functions.length === 0) {\n    return 0;\n  }\n  const json = JSON.stringify(functions);\n  return countTokens(json);\n}\n\nfunction countMessageTokens(message: Message): number {\n  if (message.content) {\n    return countTokens(message.content);\n  } else if (message.function_call) {\n    return countTokens(JSON.stringify(message.function_call));\n  }\n  return 0;\n}\n\nfunction countTokens(text: string): number {\n  return text.length / 3;\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AACA,sBAMO;;;ACPP,oBAA2E;AAEpE,SAAS,yBACd,YACA,OACA;AACA,QAAM,UAAU,IAAI,YAAY,EAAE,OAAO,WAAW,KAAK,UAAU,KAAK,IAAI,MAAM;AAClF,aAAY,QAAQ,OAAO;AAC7B;AA4CO,SAAS,uBAAuB,YAAkD;AACvF,QAAM,UAAU,IAAI,YAAY,EAAE,OAAO,kBAAkB;AAC3D,aAAW,QAAQ,OAAO;AAC5B;;;ADxCO,IAAM,mBAAN,MAA2D;AAAA,EAChE,YAAoB,SAAgE;AAAhE;AAAA,EAAiE;AAAA,EAErF,MAAM,OAAO,gBAAmD;AAlBlE;AAmBI,qBAAiB,KAAK,eAAe,cAAc;AAEnD,UAAM,SAAS,MAAM,KAAK,QAAQ,cAAc;AAMhD,QAAI,OAAO,WAAW,UAAU;AAC9B,aAAO,IAAI,0BAA0B,MAAM;AAAA,IAC7C,WAIS,aAAa,UAAU,OAAO,OAAO,YAAY,UAAU;AAClE,aAAO,IAAI,0BAA0B,OAAO,UAAS,YAAO,sBAAP,mBAA0B,UAAU;AAAA,IAC3F,WAIS,eAAe,QAAQ;AAC9B,aAAO,IAAI,2BAA0B,YAAO,cAAP,mBAAkB,UAAS,YAAO,cAAP,mBAAkB,UAAU;AAAA,IAC9F,WAIS,eAAe,QAAQ;AAC9B,aAAO,KAAK,aAAa,MAAM;AAAA,IACjC;AAIA,YAAQ,MAAM,8CAA8C;AAC5D,UAAM,IAAI,MAAM,8CAA8C;AAAA,EAChE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASQ,eAAe,gBAAqB;AAC1C,UAAM,qBAAqB,OAAO,OAAO,CAAC,GAAG,cAAc;AAG3D,QAAI,eAAe,YAAY,MAAM,QAAQ,eAAe,QAAQ,GAAG;AACrE,YAAM,cAA6B,CAAC;AACpC,iBAAW,WAAW,eAAe,UAAU;AAC7C,YAAI,QAAQ,SAAS,QAAQ;AAC3B,sBAAY,KAAK,IAAI,6BAAa,QAAQ,OAAO,CAAC;AAAA,QACpD,WAAW,QAAQ,SAAS,aAAa;AACvC,sBAAY,KAAK,IAAI,0BAAU,QAAQ,OAAO,CAAC;AAAA,QACjD,WAAW,QAAQ,SAAS,UAAU;AACpC,sBAAY,KAAK,IAAI,8BAAc,QAAQ,OAAO,CAAC;AAAA,QACrD;AAAA,MACF;AACA,yBAAmB,WAAW;AAAA,IAChC;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,aAAa,eAA4D;AACvE,QAAI,SAAS,cAAc,UAAU;AAErC,mBAAe,QAAQ,YAAgE;AACrF,UAAI,YAAY;AACd,YAAI;AACF,qBAAW,MAAM;AAAA,QACnB,SAAS,GAAP;AAAA,QAAW;AAAA,MACf;AACA,UAAI,QAAQ;AACV,YAAI;AACF,gBAAM,OAAO,OAAO;AAAA,QACtB,SAAS,GAAP;AAAA,QAAW;AAAA,MACf;AAAA,IACF;AAEA,WAAO,IAAI,eAAoB;AAAA,MAC7B,MAAM,KAAK,YAAY;AA1G7B;AA2GQ,eAAO,MAAM;AACX,cAAI;AACF,kBAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAE1C,gBAAI,MAAM;AACR,qCAAuB,UAAU;AACjC,oBAAM,QAAQ,UAAU;AACxB;AAAA,YACF;AAEA,kBAAM,aAAY,iBAAM,cAAN,mBAAiB,sBAAjB,mBAAoC;AACtD,kBAAM,WAAU,oCAAO,cAAP,mBAAkB;AAClC,kBAAM,QAA6B;AAAA,cACjC,SAAS;AAAA,gBACP;AAAA,kBACE,OAAO;AAAA,oBACL,MAAM;AAAA,oBACN;AAAA,oBACA,GAAI,YAAY,EAAE,YAAY,UAAU,IAAI,CAAC;AAAA,kBAC/C;AAAA,gBACF;AAAA,cACF;AAAA,YACF;AACA,qCAAyB,YAAY,KAAK;AAAA,UAC5C,SAAS,OAAP;AACA,uBAAW,MAAM,KAAK;AACtB,kBAAM,QAAQ,UAAU;AACxB;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,MACA,SAAS;AACP,gBAAQ;AAAA,MACV;AAAA,IACF,CAAC;AAAA,EACH;AACF;AAKA,IAAM,4BAAN,cAAwC,eAAoB;AAAA,EAC1D,YAAY,UAAkB,IAAI,WAAiB;AACjD,UAAM;AAAA,MACJ,MAAM,YAAY;AAChB,cAAM,QAA6B;AAAA,UACjC,SAAS;AAAA,YACP;AAAA,cACE,OAAO;AAAA,gBACL,MAAM;AAAA,gBACN;AAAA,gBACA,GAAI,YAAY,EAAE,YAAY,UAAU,IAAI,CAAC;AAAA,cAC/C;AAAA,YACF;AAAA,UACF;AAAA,QACF;AACA,iCAAyB,YAAY,KAAK;AAC1C,+BAAuB,UAAU;AAEjC,mBAAW,MAAM;AAAA,MACnB;AAAA,MACA,SAAS;AAAA,MAAC;AAAA,IACZ,CAAC;AAAA,EACH;AACF;","names":[]}